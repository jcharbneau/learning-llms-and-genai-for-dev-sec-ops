{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the costs we can consider caching the results of the llm.\n",
    "Langchain allows us to do global caching of calls.\n",
    "You need to verify that the caching does not contain personalized answers that should not be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-openai gptcache\n",
    "%pip install -q --no-cache-dir \"onnxruntime>=1.14.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first strategy we enable the caching on a global level.\n",
    "We use a database to store the input prompt and output prompt.\n",
    "And when we get another request that is the same , we return it from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache import Cache\n",
    "from gptcache.manager.factory import manager_factory\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "\n",
    "from langchain.cache import GPTCache\n",
    "import langchain\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def get_hashed_name(name):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def init_gptcache_exact_match(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=manager_factory(manager=\"map\", data_dir=f\"map_cache_{hashed_llm}\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def init_gptcache_embeddings_match(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    init_similar_cache(cache_obj=cache_obj, data_dir=f\"similar_cache_{hashed_llm}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same prompt 10 times is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import OpenAI\n",
    "llm=OpenAI(temperature=0)\n",
    "prompt=\"Hello world\"\n",
    "\n",
    "langchain.llm_cache=None\n",
    "for i in range(1,10):\n",
    "    result = llm.invoke(prompt)\n",
    "   # print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we enable the caching it goes a lot faster once it's warmed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = GPTCache(init_gptcache_exact_match)\n",
    "# Now run it once\n",
    "result = llm.invoke(\"Hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now run it 10 times\n",
    "for i in range(1,10):\n",
    "    result = llm.invoke(prompt)\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With embeddings we can make this is a bit more clever. Not just exact matches can be used to return, but now also make it return similar questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickdebois/Library/Caches/pypoetry/virtualenvs/learning-llms-and-genai-for-dev-sec-ops--fI-1qgv-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set the caching to use embeddings\n",
    "langchain.llm_cache = GPTCache(init_gptcache_embeddings_match)\n",
    "\n",
    "# Now run it once to warm up the cache\n",
    "result = llm.invoke(\"Hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a similar request\n",
    "similar_prompt=\"Hello world :)\"\n",
    "for i in range(1,10):\n",
    "    result = llm.invoke(similar_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "london-devops-VW7lFx7f-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
